# Evaluating Lightweight Neural Models for Edge-Based Anomaly Detection: Performance and Efficiency Trade-offs
## Usage instructions and project overview

readme_path = Path("3SLiteNet/Project_All_Files/Readme.md")
readme_content = '''
# ğŸ§  TensorFlow Lightweight Model Benchmark
This project compares lightweight neural network architectures using TensorFlow, with support for pruning, quantization, and resource profiling. Each includes baseline, optimized, and quantized + pruned versions.
### Summary of Models:
* CNN
* LSTM
* GRU
* Transformer
* CNN + GRU hybrid
* SimpleRNN

## ğŸ“ Project Structure

'''
benchmark_structured/
â”‚
â”œâ”€â”€ config.py # Shared hyperparameters and dataset path
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ main.py # CLI runner for model benchmarking
â”œâ”€â”€ dashboard.py # Dashboard for viewing modules
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ preprocess.py # SMOTE, normalization, train/test split
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ cnn_pruned.py # Baseline/Optimized/Quantized CNN
â”‚ â”œâ”€â”€ lstm_quantized.py # LSTM model variants
â”‚ â””â”€â”€ transformer_tiny.py # Transformer variants
â”‚ â”œâ”€â”€ cnn_gru_pruned.py # Baseline/Optimized/Quantized CNN_gru
â”‚ â”œâ”€â”€ gru_prune.py # GRU model variants
â”‚ â””â”€â”€ simplernn_prunes.py # simple learn variants
â”œâ”€â”€ train_eval/
â”‚ â”œâ”€â”€ train.py # Unified training function
â”‚ â””â”€â”€ metrics.py # Evaluation metrics
â”‚
â”œâ”€â”€ benchmark/
â”‚ â””â”€â”€ resource_profiler.py # Latency, model size, memory usage
â”‚
â”œâ”€â”€ logger/
â”‚ â””â”€â”€ result_logger.py # Centralized result saving to CSV
â”‚
â””â”€â”€ results/ # CSV outputs for each model


---

##  How to Run

### Step 1: Install dependencies
```bash
pip install -r requirements.txt

### Step 2: Run a specific model
python main.py --model cnn
python main.py --model lstm
python main.py --model transformer
python main.py --model gru
python main.py --model cnn_gru
python main.py --model simplernn
#### To Run the Dashboard
streamlit run dashboard.py

## Features
âœ‚ï¸ Pruning with TensorFlow Model Optimization Toolkit
ğŸ“¦ Quantization using TFLite
ğŸ§ª Standard metrics: Accuracy, Precision, Recall, F1, AUC
ğŸ“‰ Resource profiling: latency, size, memory
ğŸ“ Centralized CSV logging to results/ folder

## Output
* Each model script saves a CSV file like:
* results/tensorflow_cnn_comparison.csv
* results/tensorflow_lstm_comparison.csv
* results/tensorflow_transformer_comparison.csv

## Extend This
You can add more models like GRUs, MLPs, or MobileNet variants by dropping new scripts into models/ and updating main.py.